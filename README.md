# LLM-Finetuning

## λΉ λ¥Έ μ‹μ‘

μ•„λμ—μ„λ” π¤– ModelScopeμ™€ π¤— Transformersλ¥Ό μ‚¬μ©ν•μ—¬ Qwen-Chatλ¥Ό μ‚¬μ©ν•λ” κ°„λ‹¨ν• μμ λ¥Ό μ κ³µν•©λ‹λ‹¤.

ν™κ²½ μ„¤μ • λ‹¨κ³„μ λ€λ¶€λ¶„μ„ κ±΄λ„λ›°λ ¤λ©΄ λ―Έλ¦¬ λΉλ“λ λ„μ»¤ μ΄λ―Έμ§€λ¥Ό μ‚¬μ©ν•  μ μμµλ‹λ‹¤. μμ„Έν• λ‚΄μ©μ€ ["λ―Έλ¦¬ λΉλ“λ λ„μ»¤ μ΄λ―Έμ§€ μ‚¬μ©ν•κΈ°"](#-docker) μ„Ήμ…μ„ μ°Έμ΅°ν•μ„Έμ”.

λ„μ»¤λ¥Ό μ‚¬μ©ν•μ§€ μ•λ” κ²½μ°, ν™κ²½μ„ μ„¤μ •ν•κ³  ν•„μ”ν• ν¨ν‚¤μ§€λ¥Ό μ„¤μΉν–λ”μ§€ ν™•μΈν•μ„Έμ”. μ„μ μ”κµ¬ μ‚¬ν•­μ„ μ¶©μ΅±ν•λ”μ§€ ν™•μΈν• ν›„ μΆ…μ† λΌμ΄λΈλ¬λ¦¬λ¥Ό μ„¤μΉν•μ„Έμ”.

```bash
pip install -r requirements.txt
```

κΈ°κΈ°κ°€ fp16 λλ” bf16μ„ μ§€μ›ν•λ” κ²½μ°, λ†’μ€ ν¨μ¨μ„±κ³Ό λ‚®μ€ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ μ„ν•΄ [flash-attention](https://github.com/Dao-AILab/flash-attention) (**ν„μ¬ flash attention 2λ¥Ό μ§€μ›ν•©λ‹λ‹¤.**)μ„ μ„¤μΉν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. (**flash-attentionμ€ μ„ νƒ μ‚¬ν•­μ΄λ©° μ„¤μΉν•μ§€ μ•μ•„λ„ ν”„λ΅μ νΈλ” μ •μƒμ μΌλ΅ μ‹¤ν–‰λ  μ μμµλ‹λ‹¤**)

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# μ•„λλ” μ„ νƒ μ‚¬ν•­μ…λ‹λ‹¤. μ„¤μΉμ— μ‹κ°„μ΄ κ±Έλ¦΄ μ μμµλ‹λ‹¤.
# pip install csrc/layer_norm
# flash-attn λ²„μ „μ΄ 2.1.1λ³΄λ‹¤ λ†’μ€ κ²½μ° λ‹¤μμ€ ν•„μ”ν•μ§€ μ•μµλ‹λ‹¤.
# pip install csrc/rotary
```

μ΄μ  ModelScope λλ” Transformersλ΅ μ‹μ‘ν•  μ μμµλ‹λ‹¤.

## νμΈνλ‹

### μ‚¬μ©λ²•
μ΄μ  μ‚¬μ©μκ°€ λ‹¤μ΄μ¤νΈλ¦Ό μ• ν”λ¦¬μΌ€μ΄μ…μ„ μ„ν•΄ μ‚¬μ „ ν›λ ¨λ λ¨λΈμ„ κ°„λ‹¨ν•κ² νμΈνλ‹ν•  μ μλ„λ΅ κ³µμ‹ ν›λ ¨ μ¤ν¬λ¦½νΈμΈ `finetune.py`λ¥Ό μ κ³µν•©λ‹λ‹¤. λν• κ±±μ • μ—†μ΄ νμΈνλ‹μ„ μ‹μ‘ν•  μ μλ” μ‰ μ¤ν¬λ¦½νΈλ„ μ κ³µν•©λ‹λ‹¤. μ΄ μ¤ν¬λ¦½νΈλ” [DeepSpeed](https://github.com/microsoft/DeepSpeed)μ™€ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)λ¥Ό μ‚¬μ©ν• ν›λ ¨μ„ μ§€μ›ν•©λ‹λ‹¤. μ κ³µλ μ‰ μ¤ν¬λ¦½νΈλ” DeepSpeedλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤ (μ°Έκ³ : μ΄λ” pydanticμ μµμ‹  λ²„μ „κ³Ό μ¶©λν•  μ μμΌλ―€λ΅ `pydantic<2.0`μ„ μ‚¬μ©ν•΄μ•Ό ν•©λ‹λ‹¤)μ™€ Peft. λ‹¤μκ³Ό κ°™μ΄ μ„¤μΉν•  μ μμµλ‹λ‹¤:
```bash
pip install "peft<0.8.0" deepspeed
```

ν›λ ¨ λ°μ΄ν„°λ¥Ό μ¤€λΉ„ν•λ ¤λ©΄ λ¨λ“  μƒν”μ„ λ¦¬μ¤νΈμ— λ„£κ³  json νμΌλ΅ μ €μ¥ν•΄μ•Ό ν•©λ‹λ‹¤. κ° μƒν”μ€ idμ™€ λ€ν™”λ¥Ό μ„ν• λ¦¬μ¤νΈλ΅ κµ¬μ„±λ λ”•μ…”λ„λ¦¬μ…λ‹λ‹¤. μ•„λλ” 1κ°μ μƒν”μ΄ μλ” κ°„λ‹¨ν• μμ  λ¦¬μ¤νΈμ…λ‹λ‹¤:
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "δ½ ε¥½"
      },
      {
        "from": "assistant",
        "value": "ζ‘ζ―δΈ€δΈθ―­θ¨€ζ¨΅ε‹οΌζ‘ε«ι€δΉ‰εƒι—®γ€‚"
      }
    ]
  }
]
```

λ°μ΄ν„° μ¤€λΉ„ ν›„, μ κ³µλ μ‰ μ¤ν¬λ¦½νΈλ¥Ό μ‚¬μ©ν•μ—¬ νμΈνλ‹μ„ μ‹¤ν–‰ν•  μ μμµλ‹λ‹¤. λ°μ΄ν„° νμΌμ κ²½λ΅μΈ `$DATA`λ¥Ό μ§€μ •ν•λ” κ²ƒμ„ μμ§€ λ§μ„Έμ”.

νμΈνλ‹ μ¤ν¬λ¦½νΈλ¥Ό μ‚¬μ©ν•λ©΄ λ‹¤μμ„ μν–‰ν•  μ μμµλ‹λ‹¤:
- μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹
- LoRA
- Q-LoRA

μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹μ€ μ „μ²΄ ν›λ ¨ κ³Όμ •μ—μ„ λ¨λ“  νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈν•΄μ•Ό ν•©λ‹λ‹¤. ν›λ ¨μ„ μ‹μ‘ν•λ ¤λ©΄ λ‹¤μ μ¤ν¬λ¦½νΈλ¥Ό μ‹¤ν–‰ν•μ„Έμ”:

```bash
# λ¶„μ‚° ν›λ ¨. λ‹¨μΌ GPU ν›λ ¨ μ¤ν¬λ¦½νΈλ” μ κ³µν•μ§€ μ•μµλ‹λ‹¤. GPU λ©”λ¨λ¦¬ λ¶€μ΅±μΌλ΅ ν›λ ¨μ΄ μ¤‘λ‹¨λ  μ μκΈ° λ•λ¬Έμ…λ‹λ‹¤.
bash finetune/finetune_ds.sh
```

μ‰ μ¤ν¬λ¦½νΈμ—μ„ μ¬λ°”λ¥Έ λ¨λΈ μ΄λ¦„ λλ” κ²½λ΅, λ°μ΄ν„° κ²½λ΅, μ¶λ ¥ λ””λ ‰ν† λ¦¬λ¥Ό μ§€μ •ν•λ” κ²ƒμ„ μμ§€ λ§μ„Έμ”. λ λ‹¤λ¥Έ μ£Όμν•  μ μ€ μ΄ μ¤ν¬λ¦½νΈμ—μ„ DeepSpeed ZeRO 3λ¥Ό μ‚¬μ©ν•λ‹¤λ” κ²ƒμ…λ‹λ‹¤. λ³€κ²½ν•κ³  μ‹¶λ‹¤λ©΄ `--deepspeed` μΈμλ¥Ό μ κ±°ν•κ±°λ‚ μ”κµ¬ μ‚¬ν•­μ— λ”°λΌ DeepSpeed κµ¬μ„± json νμΌμ„ λ³€κ²½ν•λ©΄ λ©λ‹λ‹¤. λν• μ΄ μ¤ν¬λ¦½νΈλ” νΌν•© μ •λ°€λ„ ν›λ ¨μ„ μ§€μ›ν•λ―€λ΅ `--bf16 True` λλ” `--fp16 True`λ¥Ό μ‚¬μ©ν•  μ μμµλ‹λ‹¤. fp16μ„ μ‚¬μ©ν•  λ•λ” νΌν•© μ •λ°€λ„ ν›λ ¨μΌλ΅ μΈν•΄ DeepSpeedλ¥Ό μ‚¬μ©ν•΄μ•Ό ν•©λ‹λ‹¤. κ²½ν—μ μΌλ΅ κΈ°κ³„κ°€ bf16μ„ μ§€μ›ν•λ‹¤λ©΄ μ‚¬μ „ ν›λ ¨ λ° μ •λ ¬κ³Ό μΌκ΄€μ„±μ„ μ μ§€ν•κΈ° μ„ν•΄ bf16μ„ μ‚¬μ©ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ”°λΌμ„ κΈ°λ³Έμ μΌλ΅ bf16μ„ μ‚¬μ©ν•©λ‹λ‹¤.

λ§μ°¬κ°€μ§€λ΅ LoRAλ¥Ό μ‹¤ν–‰ν•λ ¤λ©΄ μ•„λμ™€ κ°™μ΄ λ‹¤λ¥Έ μ¤ν¬λ¦½νΈλ¥Ό μ‹¤ν–‰ν•μ„Έμ”. μ‹μ‘ν•κΈ° μ „μ— `peft`λ¥Ό μ„¤μΉν–λ”μ§€ ν™•μΈν•μ„Έμ”. λν• λ¨λΈ, λ°μ΄ν„°, μ¶λ ¥ κ²½λ΅λ¥Ό μ§€μ •ν•΄μ•Ό ν•©λ‹λ‹¤. μ‚¬μ „ ν›λ ¨λ λ¨λΈμ— λ€ν•΄ μ λ€ κ²½λ΅λ¥Ό μ‚¬μ©ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. LoRAλ” μ–΄λ‘ν„°λ§ μ €μ¥ν•κ³  μ–΄λ‘ν„° κµ¬μ„± json νμΌμ μ λ€ κ²½λ΅λ” λ΅λ“ν•  μ‚¬μ „ ν›λ ¨λ λ¨λΈμ„ μ°Ύλ” λ° μ‚¬μ©λκΈ° λ•λ¬Έμ…λ‹λ‹¤. λν• μ΄ μ¤ν¬λ¦½νΈλ” bf16κ³Ό fp16μ„ λ¨λ‘ μ§€μ›ν•©λ‹λ‹¤.

```bash
# λ‹¨μΌ GPU ν›λ ¨
bash finetune/finetune_lora_single_gpu.sh
# λ¶„μ‚° ν›λ ¨
bash finetune/finetune_lora_ds.sh
```

μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹κ³Ό λΉ„κµν•μ—¬ LoRA ([λ…Όλ¬Έ](https://arxiv.org/abs/2106.09685))λ” μ–΄λ‘ν„° λ μ΄μ–΄μ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈν•κ³  μ›λμ λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ λ μ΄μ–΄λ” λ™κ²°λ μƒνƒλ΅ μ μ§€ν•©λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ λ©”λ¨λ¦¬ λΉ„μ©κ³Ό κ³„μ‚° λΉ„μ©μ„ ν¬κ² μ¤„μΌ μ μμµλ‹λ‹¤.

LoRAλ¥Ό μ‚¬μ©ν•μ—¬ Qwen-7Bμ™€ κ°™μ€ κΈ°λ³Έ μ–Έμ–΄ λ¨λΈμ„ νμΈνλ‹ν•λ” κ²½μ° Qwen-7B-Chatμ™€ κ°™μ€ μ±„ν… λ¨λΈ λ€μ‹  μ¤ν¬λ¦½νΈκ°€ μλ™μΌλ΅ μ„λ² λ”© λ° μ¶λ ¥ λ μ΄μ–΄λ¥Ό ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°λ΅ μ „ν™ν•λ‹¤λ” μ μ— μ μν•μ„Έμ”. μ΄λ” κΈ°λ³Έ μ–Έμ–΄ λ¨λΈμ΄ ChatML ν•μ‹μΌλ΅ μΈν• νΉμ ν† ν°μ— λ€ν• μ§€μ‹μ΄ μ—†κΈ° λ•λ¬Έμ…λ‹λ‹¤. λ”°λΌμ„ λ¨λΈμ΄ ν† ν°μ„ μ΄ν•΄ν•κ³  μμΈ΅ν•  μ μλ„λ΅ μ΄λ¬ν• λ μ΄μ–΄λ¥Ό μ—…λ°μ΄νΈν•΄μ•Ό ν•©λ‹λ‹¤. λ‹¤μ‹ λ§ν•΄, LoRAμ—μ„ ν›λ ¨μ— νΉμ ν† ν°μ„ λ„μ…ν•λ‹¤λ©΄ μ½”λ“ λ‚΄μ—μ„ `modules_to_save`λ¥Ό μ„¤μ •ν•μ—¬ λ μ΄μ–΄λ¥Ό ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°λ΅ μ„¤μ •ν•΄μ•Ό ν•©λ‹λ‹¤. λν• μ΄λ¬ν• νλΌλ―Έν„°λ¥Ό ν›λ ¨ κ°€λ¥ν•κ² λ§λ“¤λ©΄ ZeRO 3λ¥Ό μ‚¬μ©ν•  μ μ—†μΌλ―€λ΅ κΈ°λ³Έμ μΌλ΅ μ¤ν¬λ¦½νΈμ—μ„ ZeRO 2λ¥Ό μ‚¬μ©ν•©λ‹λ‹¤. μƒλ΅μ΄ ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°κ°€ μ—†λ‹¤λ©΄ DeepSpeed κµ¬μ„± νμΌμ„ λ³€κ²½ν•μ—¬ ZeRO 3λ΅ μ „ν™ν•  μ μμµλ‹λ‹¤. λν• μ΄λ¬ν• ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°μ μ λ¬΄μ— λ”°λΌ LoRAμ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ— μƒλ‹Ήν• μ°¨μ΄κ°€ μμμ„ λ°κ²¬ν–μµλ‹λ‹¤. λ”°λΌμ„ λ©”λ¨λ¦¬μ— λ¬Έμ κ°€ μλ‹¤λ©΄ μ±„ν… λ¨λΈμ— λ€ν•΄ LoRA νμΈνλ‹μ„ μν–‰ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. μμ„Έν• μ •λ³΄λ” μ•„λ ν”„λ΅ν•„μ„ ν™•μΈν•μ„Έμ”.

μ—¬μ „ν λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•λ‹¤λ©΄ Q-LoRA ([λ…Όλ¬Έ](https://arxiv.org/abs/2305.14314))λ¥Ό κ³ λ ¤ν•  μ μμµλ‹λ‹¤. μ΄λ” μ–‘μν™”λ λ€κ·λ¨ μ–Έμ–΄ λ¨λΈκ³Ό νμ΄μ§€λ“ μ–΄ν…μ…κ³Ό κ°™μ€ λ‹¤λ¥Έ κΈ°μ μ„ μ‚¬μ©ν•μ—¬ λ©”λ¨λ¦¬ λΉ„μ©μ„ λ”μ± μ¤„μΌ μ μμµλ‹λ‹¤.

μ°Έκ³ : λ‹¨μΌ GPU Q-LoRA ν›λ ¨μ„ μ‹¤ν–‰ν•λ ¤λ©΄ `pip` λλ” `conda`λ¥Ό ν†µν•΄ `mpi4py`λ¥Ό μ„¤μΉν•΄μ•Ό ν•  μ μμµλ‹λ‹¤.

Q-LoRAλ¥Ό μ‹¤ν–‰ν•λ ¤λ©΄ λ‹¤μ μ¤ν¬λ¦½νΈλ¥Ό μ§μ ‘ μ‹¤ν–‰ν•μ„Έμ”:

```bash
# λ‹¨μΌ GPU ν›λ ¨
bash finetune/finetune_qlora_single_gpu.sh
# λ¶„μ‚° ν›λ ¨
bash finetune/finetune_qlora_ds.sh
```

Q-LoRAμ κ²½μ°, Qwen-7B-Chat-Int4μ™€ κ°™μ€ μ κ³µλ μ–‘μν™” λ¨λΈμ„ λ΅λ“ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. bf16 λ¨λΈμ„ μ‚¬μ©ν•΄μ„λ” **μ• λ©λ‹λ‹¤**. μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹ λ° LoRAμ™€ λ‹¬λ¦¬ Q-LoRAμ—μ„λ” fp16λ§ μ§€μ›λ©λ‹λ‹¤. λ‹¨μΌ GPU ν›λ ¨μ κ²½μ°, torch ampλ΅ μΈν• μ¤λ¥ κ΄€μ°°λ΅ μΈν•΄ νΌν•© μ •λ°€λ„ ν›λ ¨μ„ μ„ν•΄ DeepSpeedλ¥Ό μ‚¬μ©ν•΄μ•Ό ν•©λ‹λ‹¤. λν• Q-LoRAμ κ²½μ° LoRAμ νΉμ ν† ν° λ¬Έμ κ°€ μ—¬μ „ν μ΅΄μ¬ν•©λ‹λ‹¤. κ·Έλ¬λ‚ ChatML ν•μ‹μ νΉμ ν† ν°μ„ ν•™μµν• μ±„ν… λ¨λΈμ— λ€ν•΄μ„λ§ Int4 λ¨λΈμ„ μ κ³µν•λ―€λ΅ λ μ΄μ–΄μ— λ€ν•΄ κ±±μ •ν•  ν•„μ”κ°€ μ—†μµλ‹λ‹¤. Int4 λ¨λΈμ λ μ΄μ–΄λ” ν›λ ¨ κ°€λ¥ν•΄μ„λ” μ• λλ©°, λ”°λΌμ„ ν›λ ¨μ— νΉμ ν† ν°μ„ λ„μ…ν•λ©΄ Q-LoRAκ°€ μ‘λ™ν•μ§€ μ•μ„ μ μμµλ‹λ‹¤.

> μ°Έκ³ : Hugging Faceμ λ‚΄λ¶€ λ©”μ»¤λ‹μ¦μΌλ΅ μΈν•΄ μ €μ¥λ μ²΄ν¬ν¬μΈνΈμ—μ„ νΉμ • λΉ„ Python νμΌ(μ: `*.cpp` λ° `*.cu`)μ΄ 
> λ„λ½λ  μ μμµλ‹λ‹¤. μ΄λ¬ν• νμΌμ„ λ‹¤λ¥Έ νμΌμ΄ ν¬ν•¨λ λ””λ ‰ν† λ¦¬μ— μλ™μΌλ΅ λ³µμ‚¬ν•΄μ•Ό ν•  μ μμµλ‹λ‹¤.

μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹κ³Ό λ‹¬λ¦¬ LoRAμ™€ Q-LoRAμ ν›λ ¨μ€ μ–΄λ‘ν„° νλΌλ―Έν„°λ§ μ €μ¥ν•©λ‹λ‹¤. ν›λ ¨μ΄ Qwen-7Bμ—μ„ μ‹μ‘λλ‹¤κ³  κ°€μ •ν•λ©΄ μ•„λμ™€ κ°™μ΄ νμΈνλ‹λ λ¨λΈμ„ λ΅λ“ν•μ—¬ μ¶”λ΅ ν•  μ μμµλ‹λ‹¤:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # μ¶λ ¥ λ””λ ‰ν† λ¦¬ κ²½λ΅
    device_map="auto",
    trust_remote_code=True
).eval()
```

> NOTE: If `peft>=0.8.0`, it will try to load the tokenizer as well, however, initialized without `trust_remote_code=True`, leading to `ValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.` Currently, you could downgrade `peft<0.8.0` or move tokenizer files elsewhere to workaround this issue.

If you want to merge the adapters and save the finetuned model as a standalone model (you can only do this with LoRA, and you CANNOT merge the parameters from Q-LoRA), you can run the following codes:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # path to the output directory
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_size and safe serialization are not necessary. 
# They respectively work for sharding checkpoint and save the model to safetensors
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

The `new_model_directory` directory will contain the merged model weights and module files. Please note that `*.cu` and `*.cpp` files may be missing in the saved files. If you wish to use the KV cache functionality, please manually copy them. Besides, the tokenizer files are not saved in the new directory in this step. You can copy the tokenizer files or use the following code
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    path_to_adapter, # path to the output directory
    trust_remote_code=True
)

tokenizer.save_pretrained(new_model_directory)
```


Note: For multi-GPU training, you need to specify the proper hyperparameters for distributed training based on your machine. Besides, we advise you to specify your maximum sequence length with the argument `--model_max_length`, based on your consideration of data, memory footprint, and training speed.

### Quantize Fine-tuned Models

This section applies to full-parameter/LoRA fine-tuned models. (Note: You do not need to quantize the Q-LoRA fine-tuned model because it is already quantized.)
If you use LoRA, please follow the above instructions to merge your model before quantization. 

We recommend using [auto_gptq](https://github.com/PanQiWei/AutoGPTQ) to quantize the finetuned model. 

```bash
pip install auto-gptq optimum
```

Note: Currently AutoGPTQ has a bug referred in [this issue](https://github.com/PanQiWei/AutoGPTQ/issues/370). Here is a [workaround PR](https://github.com/PanQiWei/AutoGPTQ/pull/495), and you can pull this branch and install from the source.

First, prepare the calibration data. You can reuse the fine-tuning data, or use other data following the same format.

Second, run the following script:

```bash
python run_gptq.py \
    --model_name_or_path $YOUR_LORA_MODEL_PATH \
    --data_path $DATA \
    --out_path $OUTPUT_PATH \
    --bits 4 # 4 for int4; 8 for int8
```

This step requires GPUs and may costs a few hours according to your data size and model size.

Then, copy all `*.py`, `*.cu`, `*.cpp` files and `generation_config.json` to the output path. And we recommend you to overwrite `config.json` by copying the file from the coresponding official quantized model
(for example, if you are fine-tuning `Qwen-7B-Chat` and use `--bits 4`, you can find the `config.json` from [Qwen-7B-Chat-Int4](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4/blob/main/config.json)).
You should also rename the ``gptq.safetensors`` into ``model.safetensors``.

Finally, test the model by the same method to load the official quantized model. For example,

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("/path/to/your/model", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    "/path/to/your/model",
    device_map="auto",
    trust_remote_code=True
).eval()

response, history = model.chat(tokenizer, "δ½ ε¥½", history=None)
print(response)
```
