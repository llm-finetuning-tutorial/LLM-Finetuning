# LLM-Finetuning

## λΉ λ¥Έ μ‹μ‘

μ•„λμ—μ„λ” π¤– ModelScopeμ™€ π¤— Transformersλ¥Ό μ‚¬μ©ν•μ—¬ Qwen-Chatλ¥Ό μ‚¬μ©ν•λ” κ°„λ‹¨ν• μμ λ¥Ό μ κ³µν•©λ‹λ‹¤. λ¨Όμ € ν•„μ”ν• ν¨ν‚¤μ§€λ¥Ό μ„¤μΉν•μ„Έμ”.

```bash
pip install -r requirements.txt
```

κΈ°κΈ°κ°€ fp16 λλ” bf16μ„ μ§€μ›ν•λ” κ²½μ°, λ†’μ€ ν¨μ¨μ„±κ³Ό λ‚®μ€ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ„ μ„ν•΄ [flash-attention](https://github.com/Dao-AILab/flash-attention) (**ν„μ¬ flash attention 2λ¥Ό μ§€μ›ν•©λ‹λ‹¤.**)μ„ μ„¤μΉν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. (**flash-attentionμ€ μ„ νƒ μ‚¬ν•­μ΄λ©° μ„¤μΉν•μ§€ μ•μ•„λ„ ν”„λ΅μ νΈλ” μ •μƒμ μΌλ΅ μ‹¤ν–‰λ  μ μμµλ‹λ‹¤**)

```bash
git clone https://github.com/Dao-AILab/flash-attention
cd flash-attention && pip install .
# μ•„λλ” μ„ νƒ μ‚¬ν•­μ…λ‹λ‹¤. μ„¤μΉμ— μ‹κ°„μ΄ κ±Έλ¦΄ μ μμµλ‹λ‹¤.
# pip install csrc/layer_norm
# flash-attn λ²„μ „μ΄ 2.1.1λ³΄λ‹¤ λ†’μ€ κ²½μ° λ‹¤μμ€ ν•„μ”ν•μ§€ μ•μµλ‹λ‹¤.
# pip install csrc/rotary
```

μ΄μ  ModelScope λλ” Transformersλ΅ μ‹μ‘ν•  μ μμµλ‹λ‹¤.

## νμΈνλ‹

### μ‚¬μ©λ²•
μ‚¬μ©μκ°€ μ‰½κ² νμΈνλ‹ν•  μ μλ„λ΅ `finetune.py`λ¥Ό μ κ³µν•©λ‹λ‹¤. λν• νΈν•κ² νμΈνλ‹μ„ μ‹μ‘ν•  μ μλ” μ‰ μ¤ν¬λ¦½νΈλ„ μ κ³µν•©λ‹λ‹¤. μ΄ μ¤ν¬λ¦½νΈλ” [DeepSpeed](https://github.com/microsoft/DeepSpeed)μ™€ [FSDP](https://engineering.fb.com/2021/07/15/open-source/fsdp/)λ¥Ό μ‚¬μ©ν• ν›λ ¨μ„ μ§€μ›ν•©λ‹λ‹¤. μ κ³µλ μ‰ μ¤ν¬λ¦½νΈλ” DeepSpeedλ¥Ό μ‚¬μ©ν•©λ‹λ‹¤ (μ°Έκ³ : μ΄λ” pydanticμ μµμ‹  λ²„μ „κ³Ό μ¶©λν•  μ μμΌλ―€λ΅ `pydantic<2.0`μ„ μ‚¬μ©ν•΄μ•Ό ν•©λ‹λ‹¤)μ™€ Peft. λ‹¤μκ³Ό κ°™μ΄ μ„¤μΉν•  μ μμµλ‹λ‹¤:
```bash
pip install "peft<0.8.0" deepspeed
```

ν›λ ¨ λ°μ΄ν„°λ¥Ό μ¤€λΉ„ν•λ ¤λ©΄ λ¨λ“  μƒν”μ„ λ¦¬μ¤νΈμ— λ„£κ³  json νμΌλ΅ μ €μ¥ν•΄μ•Ό ν•©λ‹λ‹¤. κ° μƒν”μ€ idμ™€ λ€ν™”λ¥Ό μ„ν• λ¦¬μ¤νΈλ΅ κµ¬μ„±λ λ”•μ…”λ„λ¦¬μ…λ‹λ‹¤. μ•„λλ” 1κ°μ μƒν”μ΄ μλ” κ°„λ‹¨ν• μμ  λ¦¬μ¤νΈμ…λ‹λ‹¤:
```json
[
  {
    "id": "identity_0",
    "conversations": [
      {
        "from": "user",
        "value": "μ•λ…•ν•μ„Έμ”"
      },
      {
        "from": "assistant",
        "value": "μ €λ” μ–Έμ–΄ λ¨λΈμ…λ‹λ‹¤."
      }
    ]
  }
]
```

λ°μ΄ν„° μ¤€λΉ„ ν›„, μ κ³µλ μ‰ μ¤ν¬λ¦½νΈλ¥Ό μ‚¬μ©ν•μ—¬ νμΈνλ‹μ„ μ‹¤ν–‰ν•  μ μμµλ‹λ‹¤. λ°μ΄ν„° νμΌμ κ²½λ΅μΈ `$DATA`λ¥Ό μ§€μ •ν•μ„Έμ”.

νμΈνλ‹ μ¤ν¬λ¦½νΈλ¥Ό μ‚¬μ©ν•λ©΄ λ‹¤μμ„ μν–‰ν•  μ μμµλ‹λ‹¤:
- μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹
- LoRA
- Q-LoRA

### μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹
μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹μ€ μ „μ²΄ ν›λ ¨ κ³Όμ •μ—μ„ λ¨λ“  νλΌλ―Έν„°λ¥Ό μ—…λ°μ΄νΈν•΄μ•Ό ν•©λ‹λ‹¤. ν›λ ¨μ„ μ‹μ‘ν•λ ¤λ©΄ λ‹¤μ μ¤ν¬λ¦½νΈλ¥Ό μ‹¤ν–‰ν•μ„Έμ”:

```bash
# λ¶„μ‚° ν›λ ¨. λ‹¨μΌ GPU ν›λ ¨ μ¤ν¬λ¦½νΈλ” μ κ³µν•μ§€ μ•μµλ‹λ‹¤. GPU λ©”λ¨λ¦¬ λ¶€μ΅±μΌλ΅ ν›λ ¨μ΄ μ¤‘λ‹¨λ  μ μκΈ° λ•λ¬Έμ…λ‹λ‹¤.
bash finetune/finetune_ds.sh
```

μ‰ μ¤ν¬λ¦½νΈμ—μ„ μ¬λ°”λ¥Έ λ¨λΈ μ΄λ¦„ λλ” κ²½λ΅, λ°μ΄ν„° κ²½λ΅, μ¶λ ¥ λ””λ ‰ν† λ¦¬λ¥Ό μ§€μ •ν•λ” κ²ƒμ„ μμ§€ λ§μ„Έμ”. λ λ‹¤λ¥Έ μ£Όμν•  μ μ€ μ΄ μ¤ν¬λ¦½νΈμ—μ„ DeepSpeed ZeRO 3λ¥Ό μ‚¬μ©ν•λ‹¤λ” κ²ƒμ…λ‹λ‹¤. λ³€κ²½ν•κ³  μ‹¶λ‹¤λ©΄ `--deepspeed` μΈμλ¥Ό μ κ±°ν•κ±°λ‚ μ”κµ¬ μ‚¬ν•­μ— λ”°λΌ DeepSpeed κµ¬μ„± json νμΌμ„ λ³€κ²½ν•λ©΄ λ©λ‹λ‹¤. λν• μ΄ μ¤ν¬λ¦½νΈλ” νΌν•© μ •λ°€λ„ ν›λ ¨μ„ μ§€μ›ν•λ―€λ΅ `--bf16 True` λλ” `--fp16 True`λ¥Ό μ‚¬μ©ν•  μ μμµλ‹λ‹¤. fp16μ„ μ‚¬μ©ν•  λ•λ” νΌν•© μ •λ°€λ„ ν›λ ¨μΌλ΅ μΈν•΄ DeepSpeedλ¥Ό μ‚¬μ©ν•΄μ•Ό ν•©λ‹λ‹¤. κ²½ν—μ μΌλ΅ κΈ°κ³„κ°€ bf16μ„ μ§€μ›ν•λ‹¤λ©΄ μ‚¬μ „ ν›λ ¨ λ° μ •λ ¬κ³Ό μΌκ΄€μ„±μ„ μ μ§€ν•κΈ° μ„ν•΄ bf16μ„ μ‚¬μ©ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. λ”°λΌμ„ κΈ°λ³Έμ μΌλ΅ bf16μ„ μ‚¬μ©ν•©λ‹λ‹¤.

### LoRA
λ§μ°¬κ°€μ§€λ΅ LoRAλ¥Ό μ‹¤ν–‰ν•λ ¤λ©΄ μ•„λμ™€ κ°™μ΄ λ‹¤λ¥Έ μ¤ν¬λ¦½νΈλ¥Ό μ‹¤ν–‰ν•μ„Έμ”. μ‹μ‘ν•κΈ° μ „μ— `peft`λ¥Ό μ„¤μΉν–λ”μ§€ ν™•μΈν•μ„Έμ”. λν• λ¨λΈ, λ°μ΄ν„°, μ¶λ ¥ κ²½λ΅λ¥Ό μ§€μ •ν•΄μ•Ό ν•©λ‹λ‹¤. μ‚¬μ „ ν›λ ¨λ λ¨λΈμ— λ€ν•΄ μ λ€ κ²½λ΅λ¥Ό μ‚¬μ©ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. LoRAλ” μ–΄λ‘ν„°λ§ μ €μ¥ν•κ³  μ–΄λ‘ν„° κµ¬μ„± json νμΌμ μ λ€ κ²½λ΅λ” λ΅λ“ν•  μ‚¬μ „ ν›λ ¨λ λ¨λΈμ„ μ°Ύλ” λ° μ‚¬μ©λκΈ° λ•λ¬Έμ…λ‹λ‹¤. λν• μ΄ μ¤ν¬λ¦½νΈλ” bf16κ³Ό fp16μ„ λ¨λ‘ μ§€μ›ν•©λ‹λ‹¤.

```bash
# λ‹¨μΌ GPU ν›λ ¨
bash finetune/finetune_lora_single_gpu.sh
# λ¶„μ‚° ν›λ ¨
bash finetune/finetune_lora_ds.sh
```

μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹κ³Ό λΉ„κµν•μ—¬ LoRA ([λ…Όλ¬Έ](https://arxiv.org/abs/2106.09685))λ” μ–΄λ‘ν„° λ μ΄μ–΄μ νλΌλ―Έν„°λ§ μ—…λ°μ΄νΈν•κ³  μ›λμ λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ λ μ΄μ–΄λ” λ™κ²°λ μƒνƒλ΅ μ μ§€ν•©λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ λ©”λ¨λ¦¬ λΉ„μ©κ³Ό κ³„μ‚° λΉ„μ©μ„ ν¬κ² μ¤„μΌ μ μμµλ‹λ‹¤.

LoRAλ¥Ό μ‚¬μ©ν•μ—¬ Qwen-7Bμ™€ κ°™μ€ κΈ°λ³Έ μ–Έμ–΄ λ¨λΈμ„ νμΈνλ‹ν•λ” κ²½μ° Qwen-7B-Chatμ™€ κ°™μ€ μ±„ν… λ¨λΈ λ€μ‹  μ¤ν¬λ¦½νΈκ°€ μλ™μΌλ΅ μ„λ² λ”© λ° μ¶λ ¥ λ μ΄μ–΄λ¥Ό ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°λ΅ μ „ν™ν•λ‹¤λ” μ μ— μ μν•μ„Έμ”. μ΄λ” κΈ°λ³Έ μ–Έμ–΄ λ¨λΈμ΄ ChatML ν•μ‹μΌλ΅ μΈν• νΉμ ν† ν°μ— λ€ν• μ§€μ‹μ΄ μ—†κΈ° λ•λ¬Έμ…λ‹λ‹¤. λ”°λΌμ„ λ¨λΈμ΄ ν† ν°μ„ μ΄ν•΄ν•κ³  μμΈ΅ν•  μ μλ„λ΅ μ΄λ¬ν• λ μ΄μ–΄λ¥Ό μ—…λ°μ΄νΈν•΄μ•Ό ν•©λ‹λ‹¤. λ‹¤μ‹ λ§ν•΄, LoRAμ—μ„ ν›λ ¨μ— νΉμ ν† ν°μ„ λ„μ…ν•λ‹¤λ©΄ μ½”λ“ λ‚΄μ—μ„ `modules_to_save`λ¥Ό μ„¤μ •ν•μ—¬ λ μ΄μ–΄λ¥Ό ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°λ΅ μ„¤μ •ν•΄μ•Ό ν•©λ‹λ‹¤. λν• μ΄λ¬ν• νλΌλ―Έν„°λ¥Ό ν›λ ¨ κ°€λ¥ν•κ² λ§λ“¤λ©΄ ZeRO 3λ¥Ό μ‚¬μ©ν•  μ μ—†μΌλ―€λ΅ κΈ°λ³Έμ μΌλ΅ μ¤ν¬λ¦½νΈμ—μ„ ZeRO 2λ¥Ό μ‚¬μ©ν•©λ‹λ‹¤. μƒλ΅μ΄ ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°κ°€ μ—†λ‹¤λ©΄ DeepSpeed κµ¬μ„± νμΌμ„ λ³€κ²½ν•μ—¬ ZeRO 3λ΅ μ „ν™ν•  μ μμµλ‹λ‹¤. λν• μ΄λ¬ν• ν›λ ¨ κ°€λ¥ν• νλΌλ―Έν„°μ μ λ¬΄μ— λ”°λΌ LoRAμ λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ— μƒλ‹Ήν• μ°¨μ΄κ°€ μμμ„ λ°κ²¬ν–μµλ‹λ‹¤. λ”°λΌμ„ λ©”λ¨λ¦¬μ— λ¬Έμ κ°€ μλ‹¤λ©΄ μ±„ν… λ¨λΈμ— λ€ν•΄ LoRA νμΈνλ‹μ„ μν–‰ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. μμ„Έν• μ •λ³΄λ” μ•„λ ν”„λ΅ν•„μ„ ν™•μΈν•μ„Έμ”.

### Q-LoRA
μ—¬μ „ν λ©”λ¨λ¦¬κ°€ λ¶€μ΅±ν•λ‹¤λ©΄ Q-LoRA ([λ…Όλ¬Έ](https://arxiv.org/abs/2305.14314))λ¥Ό κ³ λ ¤ν•  μ μμµλ‹λ‹¤. μ–‘μν™”λ λ€κ·λ¨ μ–Έμ–΄ λ¨λΈκ³Ό νμ΄μ§€λ“ μ–΄ν…μ…κ³Ό κ°™μ€ λ‹¤λ¥Έ κΈ°μ μ„ μ‚¬μ©ν•μ—¬ λ©”λ¨λ¦¬ λΉ„μ©μ„ λ”μ± μ¤„μΌ μ μμµλ‹λ‹¤.

μ°Έκ³ : λ‹¨μΌ GPU Q-LoRA ν›λ ¨μ„ μ‹¤ν–‰ν•λ ¤λ©΄ `pip` λλ” `conda`λ¥Ό ν†µν•΄ `mpi4py`λ¥Ό μ„¤μΉν•΄μ•Ό ν•  μ μμµλ‹λ‹¤.

Q-LoRAλ¥Ό μ‹¤ν–‰ν•λ ¤λ©΄ λ‹¤μ μ¤ν¬λ¦½νΈλ¥Ό μ§μ ‘ μ‹¤ν–‰ν•μ„Έμ”:

```bash
# λ‹¨μΌ GPU ν›λ ¨
bash finetune/finetune_qlora_single_gpu.sh
# λ¶„μ‚° ν›λ ¨
bash finetune/finetune_qlora_ds.sh
```

Q-LoRAμ κ²½μ°, Qwen-7B-Chat-Int4μ™€ κ°™μ€ μ κ³µλ μ–‘μν™” λ¨λΈμ„ λ΅λ“ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤. bf16 λ¨λΈμ„ μ‚¬μ©ν•΄μ„λ” **μ• λ©λ‹λ‹¤**. μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹ λ° LoRAμ™€ λ‹¬λ¦¬ Q-LoRAμ—μ„λ” fp16λ§ μ§€μ›λ©λ‹λ‹¤. λ‹¨μΌ GPU ν›λ ¨μ κ²½μ°, torch ampλ΅ μΈν• μ¤λ¥ λ°μƒ μ‚¬λ΅€λ΅ μΈν•΄ DeepSpeedλ¥Ό μ‚¬μ©ν•΄μ•Ό ν•©λ‹λ‹¤.  

λν• Q-LoRAμ κ²½μ° LoRAμ νΉμ ν† ν° λ¬Έμ κ°€ μ—¬μ „ν μ΅΄μ¬ν•©λ‹λ‹¤. κ·Έλ¬λ‚ ChatML ν•μ‹μ νΉμ ν† ν°μ„ ν•™μµν• μ±„ν… λ¨λΈμ— λ€ν•΄μ„λ§ Int4 λ¨λΈμ„ μ κ³µν•λ―€λ΅ λ μ΄μ–΄μ— λ€ν•΄ κ±±μ •ν•  ν•„μ”κ°€ μ—†μµλ‹λ‹¤. Int4 λ¨λΈμ λ μ΄μ–΄λ” ν›λ ¨ κ°€λ¥ν•΄μ„λ” μ• λλ©°, λ”°λΌμ„ ν›λ ¨μ— νΉμ ν† ν°μ„ λ„μ…ν•λ©΄ Q-LoRAκ°€ μ‘λ™ν•μ§€ μ•μ„ μ μμµλ‹λ‹¤.

> μ°Έκ³ : Hugging Faceμ λ‚΄λ¶€ λ©”μ»¤λ‹μ¦μΌλ΅ μΈν•΄ μ €μ¥λ μ²΄ν¬ν¬μΈνΈμ—μ„ νΉμ • λΉ„ Python νμΌ(μ: `*.cpp` λ° `*.cu`)μ΄ 
> λ„λ½λ  μ μμµλ‹λ‹¤. μ΄λ¬ν• νμΌμ„ λ‹¤λ¥Έ νμΌμ΄ ν¬ν•¨λ λ””λ ‰ν† λ¦¬μ— μλ™μΌλ΅ λ³µμ‚¬ν•΄μ•Ό ν•  μ μμµλ‹λ‹¤.

μ „μ²΄ νλΌλ―Έν„° νμΈνλ‹κ³Ό λ‹¬λ¦¬ LoRAμ™€ Q-LoRAμ ν›λ ¨μ€ μ–΄λ‘ν„° νλΌλ―Έν„°λ§ μ €μ¥ν•©λ‹λ‹¤. ν›λ ¨μ΄ Qwen-7Bμ—μ„ μ‹μ‘λλ‹¤κ³  κ°€μ •ν•λ©΄ μ•„λμ™€ κ°™μ΄ νμΈνλ‹λ λ¨λΈμ„ λ΅λ“ν•μ—¬ μ¶”λ΅ ν•  μ μμµλ‹λ‹¤:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # μ¶λ ¥ λ””λ ‰ν† λ¦¬ κ²½λ΅
    device_map="auto",
    trust_remote_code=True
).eval()
```

> μ°Έκ³ : `peft>=0.8.0`μΈ κ²½μ°, ν† ν¬λ‚μ΄μ €λ„ ν•¨κ» λ΅λ“ν•λ ¤κ³  μ‹λ„ν•©λ‹λ‹¤. ν•μ§€λ§ `trust_remote_code=True` μ—†μ΄ μ΄κΈ°ν™”λμ–΄ `ValueError: Tokenizer class QWenTokenizer does not exist or is not currently imported.` μ¤λ¥κ°€ λ°μƒν•  μ μμµλ‹λ‹¤. ν„μ¬λ΅μ„λ” `peft<0.8.0`μΌλ΅ λ‹¤μ΄κ·Έλ μ΄λ“ν•κ±°λ‚ ν† ν¬λ‚μ΄μ € νμΌμ„ λ‹¤λ¥Έ κ³³μΌλ΅ μ®κ²¨ μ΄ λ¬Έμ λ¥Ό ν•΄κ²°ν•  μ μμµλ‹λ‹¤.

μ–΄λ‘ν„°λ¥Ό λ³‘ν•©ν•κ³  νμΈνλ‹λ λ¨λΈμ„ λ…λ¦½ μ‹¤ν–‰ν• λ¨λΈλ΅ μ €μ¥ν•λ ¤λ©΄ (LoRAμ—μ„λ§ κ°€λ¥ν•λ©°, Q-LoRAμ νλΌλ―Έν„°λ” λ³‘ν•©ν•  μ μ—†μ) λ‹¤μ μ½”λ“λ¥Ό μ‹¤ν–‰ν•μ„Έμ”:

```python
from peft import AutoPeftModelForCausalLM

model = AutoPeftModelForCausalLM.from_pretrained(
    path_to_adapter, # μ¶λ ¥ λ””λ ‰ν† λ¦¬ κ²½λ΅
    device_map="auto",
    trust_remote_code=True
).eval()

merged_model = model.merge_and_unload()
# max_shard_sizeμ™€ safe_serializationμ€ ν•„μκ°€ μ•„λ‹™λ‹λ‹¤. 
# κ°κ° μ²΄ν¬ν¬μΈνΈ λ¶„ν• κ³Ό λ¨λΈμ„ safetensorsλ΅ μ €μ¥ν•λ” λ° μ‚¬μ©λ©λ‹λ‹¤.
merged_model.save_pretrained(new_model_directory, max_shard_size="2048MB", safe_serialization=True)
```

`new_model_directory` λ””λ ‰ν† λ¦¬μ—λ” λ³‘ν•©λ λ¨λΈ κ°€μ¤‘μΉμ™€ λ¨λ“ νμΌμ΄ ν¬ν•¨λ©λ‹λ‹¤. μ €μ¥λ νμΌμ—μ„ `*.cu`μ™€ `*.cpp` νμΌμ΄ λ„λ½λ  μ μμΌλ‹ μ£Όμν•μ„Έμ”. KV μΊμ‹ κΈ°λ¥μ„ μ‚¬μ©ν•λ ¤λ©΄ μ΄ νμΌλ“¤μ„ μλ™μΌλ΅ λ³µμ‚¬ν•΄μ•Ό ν•©λ‹λ‹¤. λν•, μ΄ λ‹¨κ³„μ—μ„λ” ν† ν¬λ‚μ΄μ € νμΌμ΄ μƒ λ””λ ‰ν† λ¦¬μ— μ €μ¥λμ§€ μ•μµλ‹λ‹¤. ν† ν¬λ‚μ΄μ € νμΌμ„ λ³µμ‚¬ν•κ±°λ‚ λ‹¤μ μ½”λ“λ¥Ό μ‚¬μ©ν•  μ μμµλ‹λ‹¤:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(
    path_to_adapter, # μ¶λ ¥ λ””λ ‰ν† λ¦¬ κ²½λ΅
    trust_remote_code=True
)

tokenizer.save_pretrained(new_model_directory)
```

μ°Έκ³ : λ‹¤μ¤‘ GPU ν›λ ¨μ κ²½μ°, μ‚¬μ© μ¤‘μΈ κΈ°κΈ°μ— λ§λ” λ¶„μ‚° ν›λ ¨μ© ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ§€μ •ν•΄μ•Ό ν•©λ‹λ‹¤. λν• λ°μ΄ν„°, λ©”λ¨λ¦¬ μ‚¬μ©λ‰, ν›λ ¨ μ†λ„λ¥Ό κ³ λ ¤ν•μ—¬ `--model_max_length` μΈμλ΅ μµλ€ μ‹ν€€μ¤ κΈΈμ΄λ¥Ό μ§€μ •ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤.

### νμΈνλ‹λ λ¨λΈ μ–‘μν™”ν•κΈ°

μ΄ μ„Ήμ…μ€ μ „μ²΄ νλΌλ―Έν„°/LoRA νμΈνλ‹λ λ¨λΈμ— μ μ©λ©λ‹λ‹¤. (μ°Έκ³ : Q-LoRAλ΅ νμΈνλ‹λ λ¨λΈμ€ μ΄λ―Έ μ–‘μν™”λμ–΄ μμΌλ―€λ΅ μ¶”κ°€ μ–‘μν™”κ°€ ν•„μ”ν•μ§€ μ•μµλ‹λ‹¤.)
LoRAλ¥Ό μ‚¬μ©ν• κ²½μ°, μ–‘μν™” μ „μ— μ„μ μ§€μΉ¨μ— λ”°λΌ λ¨λΈμ„ λ³‘ν•©ν•΄μ£Όμ„Έμ”.

νμΈνλ‹λ λ¨λΈμ μ–‘μν™”μ—λ” [auto_gptq](https://github.com/PanQiWei/AutoGPTQ)λ¥Ό μ‚¬μ©ν•λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤.

```bash
pip install auto-gptq optimum
```

μ°Έκ³ : ν„μ¬ AutoGPTQμ—λ” [μ΄ μ΄μ](https://github.com/PanQiWei/AutoGPTQ/issues/370)μ—μ„ μ–ΈκΈ‰λ λ²„κ·Έκ°€ μμµλ‹λ‹¤. [μ΄ PR](https://github.com/PanQiWei/AutoGPTQ/pull/495)μ—μ„ ν•΄κ²° λ°©λ²•μ„ ν™•μΈν•  μ μμΌλ©°, μ΄ λΈλμΉλ¥Ό κ°€μ Έμ™€ μ†μ¤μ—μ„ μ„¤μΉν•  μ μμµλ‹λ‹¤.

λ¨Όμ €, λ³΄μ • λ°μ΄ν„°λ¥Ό μ¤€λΉ„ν•μ„Έμ”. νμΈνλ‹μ— μ‚¬μ©ν• λ°μ΄ν„°λ¥Ό μ¬μ‚¬μ©ν•κ±°λ‚ κ°™μ€ ν•μ‹μ λ‹¤λ¥Έ λ°μ΄ν„°λ¥Ό μ‚¬μ©ν•  μ μμµλ‹λ‹¤.

κ·Έ λ‹¤μ, μ•„λ μ¤ν¬λ¦½νΈλ¥Ό μ‹¤ν–‰ν•μ„Έμ”:

```bash
python run_gptq.py \
    --model_name_or_path $YOUR_LORA_MODEL_PATH \
    --data_path $DATA \
    --out_path $OUTPUT_PATH \
    --bits 4 # int4μ κ²½μ° 4, int8μ κ²½μ° 8
```

μ΄ λ‹¨κ³„λ” GPUκ°€ ν•„μ”ν•λ©°, λ°μ΄ν„° ν¬κΈ°μ™€ λ¨λΈ ν¬κΈ°μ— λ”°λΌ λ‡ μ‹κ°„μ΄ μ†μ”λ  μ μμµλ‹λ‹¤.

κ·Έ λ‹¤μ, λ¨λ“  `*.py`, `*.cu`, `*.cpp` νμΌκ³Ό `generation_config.json`μ„ μ¶λ ¥ κ²½λ΅λ΅ λ³µμ‚¬ν•μ„Έμ”. κ·Έλ¦¬κ³  ν•΄λ‹Ήν•λ” κ³µμ‹ μ–‘μν™” λ¨λΈμ—μ„ `config.json` νμΌμ„ λ³µμ‚¬ν•μ—¬ λ®μ–΄μ“°λ” κ²ƒμ΄ μΆ‹μµλ‹λ‹¤
(μλ¥Ό λ“¤μ–΄, `Qwen-7B-Chat`μ„ νμΈνλ‹ν•κ³  `--bits 4`λ¥Ό μ‚¬μ©ν• κ²½μ°, [Qwen-7B-Chat-Int4](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4/blob/main/config.json)μ—μ„ `config.json`μ„ μ°Ύμ„ μ μμµλ‹λ‹¤).
λν• `gptq.safetensors`λ¥Ό `model.safetensors`λ΅ μ΄λ¦„μ„ λ³€κ²½ν•΄μ•Ό ν•©λ‹λ‹¤.

λ§μ§€λ§‰μΌλ΅, κ³µμ‹ μ–‘μν™” λ¨λΈμ„ λ΅λ“ν•λ” κ²ƒκ³Ό κ°™μ€ λ°©λ²•μΌλ΅ λ¨λΈμ„ ν…μ¤νΈν•΄λ³΄μ„Έμ”. μλ¥Ό λ“¤λ©΄:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

tokenizer = AutoTokenizer.from_pretrained("/path/to/your/model", trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    "/path/to/your/model",
    device_map="auto",
    trust_remote_code=True
).eval()

response, history = model.chat(tokenizer, "μ•λ…•ν•μ„Έμ”", history=None)
print(response)
```